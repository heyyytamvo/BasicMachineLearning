{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with normal equation (Batch Gradient Descent)\n",
    "\n",
    "Given the dataset containing $N$ vectors $\\vec{x_i}$ corresponding to $y_i$ as below:\n",
    "\n",
    "$$D = \\{ (\\vec{x_i}, y_i) | i = 1, 2,..., N \\}$$\n",
    "\n",
    "for:\n",
    "\n",
    "$$\\vec{x_i} \\in \\mathbb{R}^{n+1}, \\vec{x_i}= \\begin{bmatrix}\n",
    "  1 \\\\\n",
    "  x_1 \\\\\n",
    "  x_2 \\\\\n",
    "  \\cdot \\\\\n",
    "  \\cdot \\\\\n",
    "  x_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then, our model $f_{\\Theta}(\\vec{x_i})$ is defined as:\n",
    "\n",
    "$$y_i \\approx ŷ_i = f_{\\Theta}(\\vec{x_i}) = \\theta_0 + \\theta_1 x^{(i)}_1 + \\theta_2 x^{(i)}_2 + \\cdot \\cdot \\cdot + \\theta_n x^{(i)}_n$$\n",
    "\n",
    "\n",
    "In linear regression, to measure the difference between predicted the value $ŷ_i$ and the actual value $y_i$, we need a Loss function $\\mathcal{L}(\\Theta, \\bar{X}, \\mathbf{Y})$, for $\\Theta$ is a set of parameters, $\\bar{X}=[\\vec{x_1}^{T},\\vec{x_2}^{T}, \\cdot \\cdot \\cdot, \\vec{x_N}^{T}]^{T}$ is the input data containing $N$ vectors $\\vec{x_i}$ and $\\mathbf{Y}=[y_1,y_2, \\cdot \\cdot \\cdot, y_N]^{T}$ is a vector containing the $y_i$ result corresponding to the $\\vec{x_i}$. Our Loss function is defined as below:\n",
    "\n",
    "$$\\mathcal{L}(\\Theta, \\bar{X}, \\mathbf{Y})=\\sum\\limits_{i=1}^N D(f_{\\Theta}(\\vec{x_i}), y_i)$$\n",
    "\n",
    "For $D(f_{\\Theta}(\\vec{x_i}), y_i)$ is the difference between the predicted value $f_{\\Theta}(\\vec{x_i})=ŷ_i$ and the actual value $y_i$. The goal in linear regression is modifying the set of arguments $\\Theta$ to minimise the value of Loss function. That action can be writen as:\n",
    "$$\\operatorname*{argmin}_{\\Theta} \\mathcal{L}(\\Theta, \\bar{X}, \\mathbf{Y})$$\n",
    "\n",
    "For our linear regression problem, the set of parameter $\\Theta$ is a vector containing $n + 1$ elements: $\\theta_0$ ,$\\theta_1$, $\\cdot \\cdot \\cdot$, $\\theta_n$.Hence, this is our goal:\n",
    "\n",
    "$$\\operatorname*{argmin}_{\\theta_0, \\theta_1, \\cdot \\cdot \\cdot , \\theta_n} \\frac{1}{2} \\sum\\limits_{i=1}^N (f_{\\Theta}(\\vec{x_i})-y_i)^2$$ \n",
    "\n",
    "Or we can write as:\n",
    "\n",
    "$$\\operatorname*{argmin}_{\\theta_0, \\theta_1, \\cdot \\cdot \\cdot , \\theta_n} \\frac{1}{2} \\sum\\limits_{i=1}^N (\\theta_0 + \\theta_1 x^{(i)}_1 + \\theta_2 x^{(i)}_2 + \\cdot \\cdot \\cdot + \\theta_n x^{(i)}_n-y_i)^2$$ \n",
    "\n",
    "### Vectorization\n",
    "\n",
    "Our loss function can be vectorized, let's define these following matrices:\n",
    "\n",
    "$$\\mathbf{\\Theta}=\\begin{bmatrix}\n",
    "  \\theta_0 \\\\\n",
    "  \\theta_1 \\\\\n",
    "  \\cdot \\\\\n",
    "  \\cdot \\\\\n",
    "  \\cdot \\\\\n",
    "  \\theta_n\n",
    "\\end{bmatrix}, \\bar{X}=\\begin{bmatrix}\n",
    "  \\vec{x_1}^{T}\\\\\n",
    "  \\vec{x_2}^{T}\\\\\n",
    "  \\cdot\\\\\n",
    "  \\cdot\\\\\n",
    "  \\cdot\\\\\n",
    "  \\vec{x_N}^{T}\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "  1 & x^{(1)}_1 & x^{(1)}_2 & \\cdot & \\cdot & \\cdot & x^{(1)}_n\\\\\n",
    "  1 & x^{(2)}_1 & x^{(2)}_2 & \\cdot & \\cdot & \\cdot & x^{(2)}_n\\\\\n",
    "  \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot\\\\\n",
    "  \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot\\\\\n",
    "  \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot & \\cdot\\\\\n",
    "  1 & x^{(N)}_1 & x^{(N)}_2 & \\cdot & \\cdot & \\cdot & x^{(N)}_n\n",
    "\\end{bmatrix}, \\mathbf{Y}=\\begin{bmatrix}\n",
    "  y_1 \\\\ \n",
    "  y_2 \\\\ \n",
    "  \\cdot \\\\ \n",
    "  \\cdot \\\\ \n",
    "  \\cdot \\\\\n",
    "  y_N\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "It is easy to see that:\n",
    "\n",
    "$$\\bar{X} \\cdot \\Theta=\\begin{bmatrix}\n",
    "  f_{\\Theta}(\\vec{x_1}) \\\\ \n",
    "  f_{\\Theta}(\\vec{x_2}) \\\\ \n",
    "  \\cdot \\\\ \n",
    "  \\cdot \\\\ \n",
    "  \\cdot \\\\\n",
    "  f_{\\Theta}(\\vec{x_N})\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\\Longleftrightarrow \\bar{X} \\cdot \\Theta-\\mathbf{Y}=\\begin{bmatrix}\n",
    "  f_{\\Theta}(\\vec{x_1}) - y_1\\\\ \n",
    "  f_{\\Theta}(\\vec{x_2}) - y_2\\\\ \n",
    "  \\cdot \\\\ \n",
    "  \\cdot \\\\ \n",
    "  \\cdot \\\\\n",
    "  f_{\\Theta}(\\vec{x_N}) - y_N\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Let $\\vec{W}=\\bar{X} \\cdot \\Theta-\\mathbf{Y}$, by Euclidean norm, we can conclude that our Loss function can be represented as:\n",
    "\n",
    "$$\\mathcal{L}(\\Theta, \\bar{X}, \\mathbf{Y})= \\frac{1}{2} \\left\\lVert \\vec{W} \\right\\rVert_2^2=\\frac{1}{2} \\left\\lVert \\bar{X} \\cdot \\Theta-\\mathbf{Y}\\right\\rVert_2^2 = \\frac{1}{2}(\\bar{X} \\cdot \\Theta-\\mathbf{Y})^{T} \\cdot (\\bar{X} \\cdot \\Theta-\\mathbf{Y})$$\n",
    "\n",
    "To minimise the value of our Loss function, let's find its derivative respect with $\\Theta$: \n",
    "$$\\frac{\\partial \\mathcal{L}(\\Theta, \\bar{X}, \\mathbf{Y})}{\\partial \\Theta}$$\n",
    "\n",
    "\n",
    "Given $x \\in \\mathbb{R}^{n}$, $b \\in \\mathbb{R}^{m}$, and $\\mathbf{A} \\in M_{m \\times n}$, we have:\n",
    "\n",
    "$$f(x)=\\left\\lVert \\mathbf{A}x - b\\right\\rVert_2^2$$\n",
    "\n",
    "and, \n",
    "\n",
    "$$\\frac{f(x)}{\\partial x}=2\\mathbf{A}^{T}(\\mathbf{A}x - b)$$\n",
    "\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\Theta, \\bar{X}, \\mathbf{Y})}{\\partial \\Theta}=\\frac{1}{2} \\times 2 \\bar{X}^{T}(\\bar{X} \\cdot \\Theta-\\mathbf{Y})=\\bar{X}^{T}(\\bar{X} \\cdot \\Theta-\\mathbf{Y})$$\n",
    "\n",
    "\n",
    "To find the local minimumm of our Loss function $\\mathcal{L}(\\Theta, \\bar{X}, \\mathbf{Y})$, we need to solve this equation:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\Theta, \\bar{X}, \\mathbf{Y})}{\\partial \\Theta} = 0\n",
    "\\Longleftrightarrow \\bar{X}^{T} \\cdot (\\bar{X} \\cdot \\Theta-\\mathbf{Y})   = 0$$\n",
    "\n",
    "$$\\Longleftrightarrow \\bar{X}^{T} \\cdot \\bar{X} \\cdot \\Theta - \\bar{X}^{T} \\cdot \\mathbf{Y}  = 0$$\n",
    "$$\\Longleftrightarrow \\bar{X}^{T} \\cdot \\bar{X} \\cdot \\Theta = \\bar{X}^{T} \\cdot \\mathbf{Y}$$\n",
    "$$\\Longleftrightarrow  \\bar{X}^{T} \\cdot \\bar{X} \\cdot \\Theta = \\bar{X}^{T} \\cdot \\mathbf{Y}$$\n",
    "\n",
    "Let $\\mathbf{A} = \\bar{X}^{T} \\cdot \\bar{X}$, our equation is expressed as:\n",
    "\n",
    "$$\\Longleftrightarrow \\mathbf{A} \\cdot \\Theta  = \\bar{X}^{T} \\cdot \\mathbf{Y}$$\n",
    "$$\\Longleftrightarrow \\Theta  = \\mathbf{A}^{-1} \\cdot \\bar{X}^{T} \\cdot \\mathbf{Y}$$\n",
    "\n",
    "Our final result is vector $\\Theta$ containing the set of approriate arguments.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
