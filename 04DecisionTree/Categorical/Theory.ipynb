{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree using ID3\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Given a random variable $\\textbf{X}$, and $\\textbf{X}$ can be one of $n$ values: $x_1$, $x_2$,...,$x_n$. Let $p_i = P(\\textbf{X}=x_i)$, $0 \\leq p_i \\leq 1$ and $\\sum_{i=1}^{n} p_i = 1$. Now, we have a terminology: **Entropy** is used to evaluate the output of a trial, the higher entropy value is, the more difficult to predict the result of a trial. Entropy value $(H(\\textbf{X}))$ of a random variable $\\textbf{X}$ is calculated in the formula below:\n",
    "\n",
    "$$H(\\textbf{X})=-\\sum_{i=1}^{n}p_i \\ln{p_i}$$\n",
    "\n",
    "and by using Larange Multipliers, we can have a conclustion:\n",
    "\n",
    "$$H(\\textbf{X})\\text{ reaching maximum value if and only if: } p_1 = p_2 = \\cdot \\cdot \\cdot = p_n = \\frac{1}{n}$$\n",
    "\n",
    "For example, given a box containing: 5 red balls, 3 green balls, 1 blue balls. Let $\\textbf{X}$ be the random variable of picking a (red/green/blue) ball, so $\\textbf{X}=(x_r, x_g, x_b)$. Then:\n",
    "\n",
    "$$H(\\textbf{X})=-\\frac{5}{9}\\ln{(\\frac{5}{9})} -\\frac{3}{9}\\ln{(\\frac{3}{9})} -\\frac{1}{9}\\ln{(\\frac{1}{9}) \\approx 0.93}$$\n",
    "\n",
    "On other hands, given a box containing: 3 red balls, 3 green balls, 3 blue balls. Let $\\textbf{X}$ be the random variable of picking a (red/green/blue) ball, so $\\textbf{X}=(x_r, x_g, x_b)$. Then:\n",
    "\n",
    "$$H(\\textbf{X})=-\\frac{3}{9}\\ln{(\\frac{3}{9})} -\\frac{3}{9}\\ln{(\\frac{3}{9})} -\\frac{3}{9}\\ln{(\\frac{3}{9}) \\approx 1.09}$$\n",
    "\n",
    "By observing, in the first case, it is more likely to pick a red ball, however, in the second case, we have a fair probability value of picking a red/green/blue ball, which mean that it is difficult to predict the color of the chosen ball.\n",
    "\n",
    "In contrast, given a box containing 7 yellow balls and 0 red ball. Let $\\textbf{X}$ be the random variable of picking a (red/yellow) ball, so $\\textbf{X}=(x_r, x_y)$, hence:\n",
    "\n",
    "$$H(\\textbf{X})=-\\frac{7}{7} \\ln{(\\frac{7}{7})}=0$$\n",
    "\n",
    "In this case, our box is considered as pure. That is also the basic idea of decision tree, using the entropy value to evaluate a purity of a dataset for classification.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
