{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid function\n",
    "\n",
    "$$S(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "# Logistic Regression\n",
    "\n",
    "Logistic regression tries to find the output of this formula:\n",
    "\n",
    "$$S(f_{\\Theta}(\\vec{x_i}))=\\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 x^{(i)}_1 + \\theta_2 x^{(i)}_2 + \\cdot \\cdot \\cdot + \\theta_n x^{(i)}_n)}}$$\n",
    "\n",
    "where,\n",
    "\n",
    "$$f_{\\Theta}(\\vec{x_i}) = \\theta_0 + \\theta_1 x^{(i)}_1 + \\theta_2 x^{(i)}_2 + \\cdot \\cdot \\cdot + \\theta_n x^{(i)}_n= \\vec{\\theta}^{T} \\vec{x_i}, \\vec{\\theta}=\\begin{bmatrix}\n",
    "  \\theta_0 \\\\\n",
    "  \\theta_1 \\\\\n",
    "  \\theta_2 \\\\\n",
    "  \\cdot \\\\\n",
    "  \\cdot \\\\\n",
    "  \\theta_n\n",
    "\\end{bmatrix}, \\vec{x_i}=\\begin{bmatrix}\n",
    "  1 \\\\\n",
    "  x^{(i)}_1 \\\\\n",
    "  x^{(i)}_2 \\\\\n",
    "  \\cdot \\\\\n",
    "  \\cdot \\\\\n",
    "  x^{(i)}_n\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "There are two cases:\n",
    "\n",
    "$$\\begin{cases}\n",
    "    \\hat{y_i} = 1 & \\text{if $S(f_{\\Theta}(\\vec{x_i})) \\geq 0.5$} \\\\\n",
    "    \\hat{y_i} = 0 & \\text{if $S(f_{\\Theta}(\\vec{x_i})) \\lt 0.5$}\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $\\hat{y_i}$ is the classified output of data input $\\vec{x_i}$\n",
    "\n",
    "This is our goal: Given a new $\\vec{x_i} \\in \\mathbb{R}^{n + 1}$, we want to find an approriate $\\vec{\\theta}$ above in order to maximize the value of $S(f_{\\Theta}(\\vec{x_i}))$, where $0 \\leq S(f_{\\Theta}(\\vec{x_i})) \\leq 1$. \n",
    "\n",
    "## Loss function\n",
    "\n",
    "Considering each data input $\\vec{x_i}$ in the training set:\n",
    "\n",
    "$$\\begin{cases}\n",
    "    P(y_i = 1|\\vec{x_i}, \\vec{\\theta})=S(\\vec{\\theta}^{T} \\vec{x_i}) = p_i \\\\\n",
    "    P(y_i = 0|\\vec{x_i}, \\vec{\\theta})= 1-S(\\vec{\\theta}^{T} \\vec{x_i}) = 1-p_i\n",
    "\\end{cases}$$\n",
    "\n",
    "Hence, at each $\\vec{x_i}$, the cost function for a single value is:\n",
    "\n",
    "$$\\begin{cases}\n",
    "    -\\log(p_i) & \\text{if $y_i=1$ } \\\\\n",
    "    -\\log(1-p_i) & \\text{if $y_i=0$}\n",
    "\\end{cases}$$\n",
    "\n",
    "### Why $-\\log(x)$\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../Images/LogisticLog.png\" alt=\"Alt Text\" width=300 height=auto>\n",
    "</p>\n",
    "\n",
    "In the picture above, we can see the function $y=-\\log(x)$ grows very large when $x$ approaches $0$ and if $x$ approaches $1$, the function is close to $0$, or:\n",
    "\n",
    "$$\\begin{cases}\n",
    "    \\lim_{{x \\to 0}} -\\log(x) = \\infty \\\\\n",
    "    \\lim_{{x \\to 1}} -\\log(x) = 0\n",
    "\\end{cases}$$\n",
    "\n",
    "### General Formula for Loss Function\n",
    "\n",
    "The loss function for the logistic regression model at every input $\\vec{x_i}$ is:\n",
    "\n",
    "$$\\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)=-y_i\\log(p_i)-(1-y_i)\\log(1-p_i), \\text{where}:$$\n",
    "\n",
    "$$y_i=0, 1; p_i=S(\\vec{\\theta}^{T}\\vec{x_i})$$\n",
    "\n",
    "## Loss function optimization\n",
    "\n",
    "### Derivative of the loss function\n",
    "Firstly, we need to find the derivative of $\\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)$ with respect to $\\vec{\\theta}$ (the details are not covered in here, if you want to know how to calculate this derivative, please check the section below):\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)}{\\partial \\theta}=(S(\\vec{\\theta}^{T}\\vec{x_i}) - y_i)\\vec{x_i}$$\n",
    "\n",
    "### Calculating Derivative\n",
    "\n",
    "$$\\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)=-y_i\\log(p_i)-(1-y_i)\\log(1-p_i) \\text{, then:}$$\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)}{\\partial \\theta}=\\frac{\\partial \\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)}{\\partial p_i} \\cdot \\frac{\\partial p_i}{\\partial \\theta}$$\n",
    "\n",
    "$$\\Longleftrightarrow \\frac{\\partial \\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)}{\\partial \\theta} = \\frac{p_i - y_i}{p_i(1-p_i)} \\cdot \\frac{\\partial}{\\partial \\theta} S(\\vec{\\theta}^{T}\\vec{x_i})$$\n",
    "\n",
    "$$\\Longleftrightarrow \\frac{\\partial \\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)}{\\partial \\theta} = \\frac{p_i - y_i}{p_i(1-p_i)} \\cdot S(\\vec{\\theta}^{T}\\vec{x_i})(1- S(\\vec{\\theta}^{T}\\vec{x_i}))\\vec{x_i}$$\n",
    "\n",
    "\n",
    "$$\\Longleftrightarrow \\frac{\\partial \\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)}{\\partial \\theta} = (S(\\vec{\\theta}^{T}\\vec{x_i}) - y_i) \\cdot \\vec{x_i}$$\n",
    "\n",
    "### Optimization with Stochaic Gradient Descent (SGD)\n",
    "\n",
    "Given a dataset with $\\textbf{N}$ $\\vec{x_i}$, to find $\\vec{\\theta}$, the Stochaic Gradient Descent performs these following steps below:\n",
    "\n",
    "1. Assigning the number of epochs, intialize $\\vec{\\theta}$ randomly\n",
    "2. At each epoch:\n",
    "\n",
    "    2.1 Shuffle $\\textbf{N}$ samples\n",
    "    \n",
    "    2.2 Iterating every $\\vec{x_i}$ and update $\\vec{\\theta}$:\n",
    "\n",
    "    $$\\vec{\\theta}_{n+1} = \\vec{\\theta}_{n} - \\eta \\frac{\\partial \\mathcal{L}(\\vec{\\theta}, \\vec{x_i}, y_i)}{\\partial \\theta}, \\eta \\text{ is learning rate}$$\n",
    "\n",
    "Doing so until the last epoch through, we achieve the $\\vec{\\theta}$\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
